Unknown Speaker  0:23  
Knock just mentioned like this idea of like aI class why do you think that St. John

Unknown Speaker  0:55  
I think it's there's very little about it that is ultra realistic. I think I think in the age of the internet so much is optics and reputation also, you know, with with call out culture some of which is entirely justified some of which is people looking for other people to bully some of the criticisms or call out culture justified some of the criticisms or call out culture just being made by people who want to protect their own horrible behavior but people are capable of anything everything

Unknown Speaker  1:31  
Thank you system binds to it, why does it Why does the conveyance conceal that because it's technology

Unknown Speaker  1:47  
that's the very thing we're going to talk about after the break. I promise you so the the insofar as bias and all bias means is nice, kind of intuitional stereotype that you're bringing with you to optimize your decision making it's not necessarily negative.

Unknown Speaker  2:27  
Technology is.

Unknown Speaker  2:37  
Typical people people are talking past each other, really? Well. On the one hand, people say no, for for something that have moral salience. For it to be say a moral action needs to be performed by a moral agent, there needs to be an intention behind it, there needs to be some kind of motivation. That's because moral agency entails moral responsibility. And this is why people with cognitive deficits or young children are not held morally responsible for their actions. And insofar as you think moral responsibility is tied really closely with with moral value, in some sense, well, then if an artifact like like a machine and an artifact, a construct, it has no self directed agency, so it can't have any responsibility because it's not a moral agent. So it's, it's it's it's morally neutral in in that sense. But that's not to say that I can't have any moral or or value laden effect on others or that it is not itself something that has emerged from from value laden behaviors but the discussion is often had by people who

Unknown Speaker  4:07  
are who don't simply buy cool cool okay, yeah, this is honestly so it might

Unknown Speaker  4:16  
be patience to realize writing in my years like the way you are

Unknown Speaker  4:27  
structure that you regardless of whether people will be converted.

Unknown Speaker  4:34  
Wow. And like, even like when I'm thinking like, my spare time and recycling is even printed out. He's like,

Unknown Speaker  4:49  
Yeah, even if it's real isn't a good thing. Yeah. Makes sense

Unknown Speaker  4:53  
that you can hire actors when you want it. The semi nature solid? Yes. It's just

Unknown Speaker  5:16  
about whether you should be struggling to get the right response there is no, it's an important question. It's a difficult one. There motivates, and I can recall right now, you know about well, you know, we need to optimize our decision making right here. And to do that, we need to make a whole bunch of assumptions. And of course, the assumptions are fallible. Now, but now we're balancing off the success rate of our predictions on the one hand, with whatever structures it is that those assumptions might be reinforcing, and the society in which we live, and maybe we don't want to reinforce those, those assumptions, maybe. Maybe it's not even trade off isn't really you're struggling with that sort of thing.

Unknown Speaker  6:07  
Yeah. And MSAD is something that we've just, we invented, we're using something that finds immutable, like the core of a person as being doable on one hand in our life.

Unknown Speaker  6:34  
And also, I wish I wish I knew, yeah, I was just hearing this idea, says,

Unknown Speaker  6:42  
they're all

Unknown Speaker  6:46  
with all of this is, is bracketed by this, the need for us to impose some sort of system of pacification on the world in order to navigate. You can't. And we wouldn't even want to just engage with the world every day. Like it's an entire, like it fell out of the sky, it's entirely new. None of our previous experiences matter, you know, we've hopefully with with with, here's a great example, right? plagiarized essays. Yeah, and I don't mean, once we pay someone else to write them, that's really hard. But once we're someone who's copying stuff, from different places, now, it will be absolutely impossible for me to list the criteria that I use to find that. But I know that when I see it, right, just just from decades of staring at it, and just knowing that there was the difference between like a professionally written academic paper, on the one hand, an undergraduate essay is roughly a decade of experience. So when you get one that simply, you just know, you're halfway through the first paragraph. Yeah. And you can tell straight away. Now, of course, that's a bias that I bring with me insofar as expectation is that undergraduate essays and everything I can say? Am I reinforcing something there that I shouldn't be reinforcing? was being left bias with me? Maybe, but I'm also mitigating something that I want to know. Yeah, now. We should never stop worrying about Yeah, yeah, that's the point of moral philosophy is that we don't just look to a bunch of pencils going, Have you followed the principle, we're good, we're absolved responsibility. Let's go to the pub. The core of moral philosophy is that we remain in that struggle. And we're always questioning ourselves in interrogating our own conclusions.

Unknown Speaker  8:45  
And just in terms of practical advice, not to sad, the Gospel about that.

Unknown Speaker  8:50  
But I'm struggling to find out we.

Unknown Speaker  9:06  
Never had a few depths criticizing, ever. The trick is always to give reasons for your criticisms is good, everyone, unless there's good essay advice. For literature review, if you're doing a thesis, and you got to do a big literature review, even for your essays, it was like the first half is a literature review. You earn your license by expounding their position and their argument properly displaying it like demonstrating that you understand it. Yeah. And then you attack it and you give reasons for your criticisms. And then the best thing to do is then to try and imagine and articulate a really good response from the from the author. So the author might respond or defender of the author's position might respond as follows. And then you say answer this, I say, X, Y, and Z. So you're trying to cover both floors of the debate. Now if you're really good about this, and really good at this, and you're really honest You might end up talking yourself out of your own belief that and that can happen, but that's okay. It's better to be honest. You see our own our own beliefs and our reasons for them. They are a data set that we're testing through our own methods of interrogation. I say, You know what, I just, I just tested the sample. And it's, for example, like the whole Sorry, I'm taking, what time is getting passed? I'm sorry. It's quarter past. Exactly. All right. I'm having to seek out others. Yeah, everyone, please remind me this poll lecturer who comes in afterwards is really polite once we stand around for 10 pass, and I'm eating chocolate answering questions. And then I realized that I'm taking up somebody else's lecture time, push me out the door, this that would be pretty fantastic. All right. What do we have a line to the circle? Okay. All right. Excellent. So firstly, now we've come back to the break. I want to say something, before we get going about the essay, your first essay, remember that my consultation hours are 12 to two on Thursdays in room 212. And building case 17. You don't need to make an appointment. I'll just be there. If you can't be there in person just flicked me a message or an email somewhere. And I'll jump on to zoom. I'm happy to discuss any aspect. I'm going to turn those consultations instance, select essay workshops as well. There's so many. It's easier to do it like that. I think they're not. They're not exactly over booked at the moment. So for all essay writing advice, and so on, please do come by, or come on to zoom in those of those hours don't work for you, then just let me know. And we'll work something out. I have a much, much more essay advice I'm going to be putting online. So rolling advice. A lot of it's just directed by you and the sort of questions that I'm getting online and on Ed. I think for the really, if I see a lot of questions repeated because everyone's so shy to ask questions about the essay publicly, everyone's sending me PMs on end with essay questions. And they're all the same. Like all the questions are the same. So I'm going to end up doing a quick mental model of all the most popular ones and I'll stick those up in a pinned comment, as well. But we're all rolling essay sessions as we go. Okay. I'll let you get his paper that is absolutely fantastic. It's it is the it is the mark of how good Vedantic philosophy should be done. When I got comments back on my what is called a

Unknown Speaker  12:42  
confirmation of status exam got when I was doing my doctorate. The the assessor Dorothy Edgington was very clever, wrote back and said, the candidates sample chapter was hard going and rather tedious. Now I took that as the world's biggest compliment. You might think that electric piano this paper is hard going and rather tedious. There's a lot of there's a lot of sort of, okay, well, here are three reasons to do this. And here are three reasons against it. And here are three and there seems to always be three or four. And it's very, very formulaic. But it's basically like doing a long mathematics problem with words. That's what bat is trying to do here. It's a mark of how a very good theoretical research analysis is done. It's incredibly slow moving, and it takes nothing for granted. You know, bat will never use a word like it clearly. Or obviously, if you ever see that in a paper, if you ever catch yourself writing, clearly, you know why you're doing that, because you don't know how to argue properly for the point you want to make. So you just write clearly and you're daring a reader to say, You know what, I don't understand this at all. This is not even vaguely clear or obvious to me, right? It's like a dead. Your bat doesn't do this. This is a very, very carefully revised paper. Now, it has two main concerns, ethics washing, an ethics bashing, we finished the last lecture discussing discussing ethics washing, even though we didn't use that term. So now what is ethics washing? Ethics washing is the weaponization of moral language by companies in order to deflect or defend against regulation that would impose enforceable constraints on their behavior. So ethics washing, it's like green washing was some company says, in our company, we no longer use plastic straws, but you know, they're like a petroleum company or something that they it's immeasurably I'm distinct. Now what's what's it ethics bashing. Ethics bashing is the reduction of moral philosophy to ethics washing. So now we know what ethics washing is ethics washing is the CO option of ethical or moral language. And then using it to get away without actually being morally answerable for any of your behaviors and to stop desperately, the government imposing any rules or regulations that might affect the way that your company conducts its own business. So you just released a whole bunch of statements about you know, your moral commitments and these sorts of things. And then when everyone's busy with the next news cycle, you just carry on doing whatever it is that you were doing before, like electrocuting Danny was something terrible like that. Now, ethics bashing, is the criticism of ethics and capital letters or moral philosophy, under the assumption that what moral philosophy really is, is this nonsense that goes on with the the endless generation of moral ethical principles. Ethics bashing is what happens when we confuse actual moral or ethical debates and discussions on the one hand, with ethics washing on the other. Now, right, in the first lecture, I said to all of you that I was going to use the terms, you know, ethics and morals or ethics and morality synonymously, I was going to use them to mean exactly the same thing. I was going to use them interchangeably. And I said, usually people do use them to mean exactly the same thing. However, if anybody ever did intend them to have a different meanings, that they would tell you that they are using them differently. Now, here's the test is bat using ethics and morality to mean the same thing or different things that soon as read the paper? 50% chance somebody take a wild Hail Mary pass fool. Yep. Different? Yes, yes, he is using them to mean different things. So pray, tell, what are these differences? Ah, honesty is honesty is a virtue, right? Does anyone know what the what the differences are, and we have these buried in there in a paragraph somewhere, but it's important.

Unknown Speaker  17:33  
We actually is using ethics to mean the practice of generating ethical principles by tech and AI companies. BSE is using the term ethics to refer to a certain corporate practice of managing public relations. She is reserving the term ethics to just mean the stuff that goes on in ethics washing on the one hand, and is using instead of ethics, the terms morality, or moral philosophy, to refer to what it is that we've been doing together here for the last month, with forgetting sums up, I think, rather wonderfully. As you know, moral philosophy is a mode of inquiry. A mode of inquiry just means a way of understanding things, a mode of inquiry, a way of understanding things, what sort of things moral things remember how we started all of this for the first week and second week, we have ways of understanding the reasons or motivations or that we might have for our moral ethical judgments or stances in terms of either utilitarianism, or rule based slash Kantian ethics or virtue ethics, by the all three of these normative moral theories, loads of understanding of moral beliefs and our own moral judgments. And bat is reserving morality and moral philosophy, those terms for this practice and using ethics to talk about a corporate exercise in public relations. I'm probably not going to do that here. I'm not going to change the way that I've been using the terms. But you need to know case you read the newspaper quickly and you mark stuff off. If you're not aware of this right at the beginning. And if you happen to miss that passage, that he should have put it in a heading or something. Made a bigger deal out of it, I think, then the paper won't make any sense when you get really, really confused. So I do want to make a big deal out of it. Here now so that your job is a bit easier when you when you when you get on in there. Now. He makes a distinction between

Unknown Speaker  19:52  
what she calls

Unknown Speaker  19:55  
an instrumental use of ethical or moral reasoning, and an intrinsic use. Now, an intrinsic use of morality or moral reasoning, sees the acts of moral reasoning, the practice of moral philosophy itself, to have moral value. The practice of moral philosophy or moral reasoning, the act of working out what your reasons are for your own moral judgments is itself something that has moral value. That's the intrinsic understanding of morality. And instrument instrumental understanding of morality is one that understands it to not have any intrinsic moral value itself. But rather, it can deliver other things that may or may not have moral value themselves, but they're things that we want, like we might want, for example, just the government to get off our backs. And let us keep making facial recognition technology that detects queer people or something like that. Right, in which case, you know, moral philosophy, like ethics, corporate ethics is great, not because it has any moral value, but because it has a pragmatic corporate value when it comes to our own sort of corporate, our own corporate interests. Now, this this, this is the distinction that we had he made and made, he's going to return to this distinction at the very at the very end of the paper, there are three main goals that bat has, in in this paper, this is pretty long for an academic paper. The first is bat wants to locate the weaknesses of ethics, washing and ethics, that fashion. We he is a careful, thorough thinker, she's not happy to just say, look at ethics, washing, and ethics bashing are bad things. So let's work out what we can do about it. She's not prepared to make that assumption. Instead, she's going to do the very thing that I've asked all of you to do in your essays. Viet, he is going to give reasons for us to believe that she is correct. When she says that ethics washing and ethics bashing are dangerous, yet, he's going to give us reasons. And the first goal about his paper is to is to give us good reasons. That's the first goal I told you, it was good paper. Now, the second goal is to look more closely at the connection between actual real valuable moral philosophy or moral reasoning on the one hand, and technology on the other and to make more of these technology research on the other, and to make more of these connections explicit. And the third goal is to give us all reasons to be less instrumental when it comes to our moral reasoning, and to realize the moral value, or appreciate the moral value of moral reasoning about the nature of technology itself. So these are the three goals. So now, the first one, ethics versus moral philosophy. We've discussed this already. It's the distinction between a corporate public relations exercise on the other and a way of understanding the reasons that we have for our own moral judgments on the other. These are not the same thing at all, a bat puts this in terms of the fact value distinction, who in their tutorials has encountered the fact value distinction, or the normative prescriptive distinction? Anything put your hand up if you have? Okay, I'll make this super clear. Here. Now.

Unknown Speaker  23:48  
Ross's green, snow is white. Grass is not green, whatever. Notice,

Unknown Speaker  24:09  
on the surface, at least, they're older statements of fact, about how things are the statements of facts. And if we're disagreeing about it, we can probably just examine the world and that will be enough to adjudicate between disagreement. get where I'm going with this. Hopefully, it's not supposed to be a trick question. And then statements of value are distinct from statements of fact. Morphing things on the lines of lying is broad.

Unknown Speaker  24:44  
Now,

Unknown Speaker  24:47  
it might look dramatically very simple. It's very similar to graft in screen lying is wrong. The difference is, there doesn't seem to be at least on the surface, any way of adjudicating between this disagreements about lying being wrong, that will not require us to display our values to the third party with whom we're disagreeing. These are statements of fact. And statements of value. Is anyone even slightly hazy on? On what this distinction is leaving aside a perspective, the very fact that you might be uttering one of these statements might be something that emerges from a system of values or something, but let's not get to this model. Right? This is just supposed to be really simple elementary stuff. None of this is a trick question. But Big Thanks, spirit. His point is that moral philosophy, as distinct from moral reasoning, as distinct from a lot of reasoning in the sciences, is designed to give us evidence, evidence in the form of articulating or reasons that can be understood and assessed by a third party for why it is that we should believe certain statements of value as opposed to others. That's the point. So you have to go and look at something under a microscope and say, Look at down the wrongness, the issue of this moral debate is being sold. We've got our wrongness detectors. No, the point is that you need to give your your reasons these betray your values and the extent to which you can convince others that those values are the right ones to have that you'll get something like consensus. What bat wants to do here, this is something that we're talking about last week, and Flora was here to build that sound expensive. Sound like that? Now, here's something for you. Oh, my God was that 2000 dogs. The concern here is how to tie properly, concerns about matters of value to concerns about the nature of tech, and AI, and so on, in a way that doesn't allow these concerns to be simply washed away by the practice of ethics washing, and doesn't allow anyone to say without giving reasons, that there are no values inherent in, in technology, or in AI. Now, I found a bit of a segue here. You'll a lot of people in this debate are talking past each other, I think you'll when it comes to the question of Oh, does technology have a value? I mean, it's not as if there's a sense in which it does in the sense in which it does. And if you think that having a value means having agency and being able to give reasons for your values and being able to take moral responsibility for your own actions. Well, then no, no artifact, is value laden at all. But that's probably not what people mean, when people ask the question, people are asking whether or not the technology has emerged out of a system of values. And the differences in values can affect a difference in existing technology. And whether or not existing technology AI are otherwise. Right, and have affects causal effects on us on our lives, that do affect things that on which we place value, in which case the answer has been the Yes. So you need to adjudicate between these two conversations that intersect at various points. One of the things that bat is arguing in the first section of our paper is that proper attention to moral philosophy, as opposed to just ethics washing, will enable us to navigate this point of intersection with greater accuracy that we might be able to otherwise. Now. One of the great things according to bat that moral philosophy will allow us to do is to escape this sort of primal intuition that something must be bad morally speaking of the wrong thing, because I just happen to feel that it is. Instead again, we'll be forced to give reasons for our moral judgments in a way that might allow us to escape what is otherwise the very shouty world of ideology. Who's ever been on Twitter. Right. Wow. Now, Twitter is a place where discussions have a tendency to go wrong, right.

Unknown Speaker  29:47  
It's a platform of discussion underpinned by a technology, a technology with a very deliberate design. A design that constrains you from articulating in any great subtlety or depth, the reasons that you have anything that you are saying, because you have a very short word count, rubbish essays can be like that. Ah, there's a great who wants to make a lot of money? Wow. Okay, I just want 5%. If you do this, this is a great idea. Roll out some sort of platform through which students write their essays, right. It's a bit like an old like a, you know, when you use word now, it's the license, it's online, you're doing it in the cloud. So things like Google Docs or something that's on the cloud. But the convener of the unit of study, or the course or the lecturer in charge can impose the word count. And when the student gets, it's like Twitter, when the student gets to the end, it just, it just turns red and said, you know, this is going to be deleted when you submit the thing. The first person to get that out there is going to make a sweet fortune. I promise, if we're able to impose the word tau on on the way in, wow, maybe I will have that idea. Okay. Just popped into my head. Now, one of the criticisms of actual, the actual practice of what we're doing here, in, in this unit of study by those who practice ethics, Washington say, look, all of this stuff about worrying about virtue ethics, and Aristotle, and Confucius and stuff, all this stuff about utilitarian, utilitarian, usually, utilitarianism usually gets off the hook, because Peter Singer is utilitarian, and he's really trendy right now. So it almost seems to be bad that people are looking after you on Twitter. All this stuff is fine for people at universities, right for people writing essays, I have all this time to do all of the stuff. But out here in the real world, where we're busy trying to beat our competitors. And we're busy trying to make money for our shareholders and everything. We have to we don't have the time to engage in this sort of long winded reflective, self aware, argumentative debates about the reasons that we might have, or we might not have for any of the moral claims that we put forward. I mean, we've just got to put them forward and get on with our job. And be I think his response to this is that insofar as they're being purely descriptive, remember the distinction purely descriptive? They're absolutely right. Yeah, actual moral reasoning. The actual practice of designing your own thought experiments that are designed to test the moral judgments that you find attractive, or for that matter that you find unattractive, that is very time consuming. It's not something that can be done quickly. No experiment to be designed quickly or executed quickly. All these things take time. But then the ad says the following astonishing thing, it bloody well is supposed to take a long time. And if it's not, and you're trying to get it done, on the Friday afternoon in between the board meeting and where it is that you and your your colleagues on the board of directors of this company go to the nightclub or something you're not spending long enough doing.

Unknown Speaker  33:34  
If your company is going to announce or put forward publicly a bunch of more principles that you do take to be guiding principles for the behavior of your company or of your r&d program or other employees, or of your entire research sector. If you're actually serious about this, then what you do need is to do it more slowly. And the debates that we're having about the morality or otherwise, of various types of technology are not the sort of things that should be reduced to easy sloganeering. It is the very thing that we should be submitting to careful, slow, reflective, sustained moral analysis of the of the sort that you will be doing when you're writing your essays, namely, the provision of good reasons for one's moral judgments. No one said this was going to be easy. In case you haven't noticed, working out what the right thing to do is morally speaking is not easy. It's really hard. It's really hard people civilizations struggled with it for 1000s and 1000s of years. What made you think you could or what made you think you could reduce it to some wash lumens and why would you want so here in this part of the paper ethic, bat concedes to the allegation, and this is but actually that's a virtue, not a detraction of the of the practice of moral philosophy. Now, second thing, moral philosophy and technology. If we move beyond, beyond very narrow concerns of procedural fairness, and instead of, you know, worsening ideological conflicts

Unknown Speaker  35:30  
we might then.

Unknown Speaker  35:33  
So the reason that we stop worsening ideological conflicts is because ideological conflicts are things that emerge when people are not taking their time to explain things slowly. This is why people on Twitter end up threatening each other with death. And so because it's very, it's very easy to find a point of contention and point of split. What the idea is hoping for here is that the very discussion about the nature of morality, surrounding technology research itself, on the platforms where these things are carried out, might, if they carried out more slowly, in terms of moral philosophy and moral reasoning, might actually give everyone the opportunity to listen to each other, instead of continuing in our usual sort of shouting away. But now I want to do a thought experiment with all of you. So that's the background to be at. Bat ends with this reasons to be less instrumental, less sloganeering. And more nuanced. And remember how at the end of the previous lecture, I suggested that there are some technologies that rather than tweaking, we might not want to research and develop in the first place. Now, who is aware, the university just at this university, just made a press release about this very thing? Who is aware of the the use of artificial intelligence in criminal sentencing? Who has read about this? Who has never heard of this before?

Unknown Speaker  37:21  
Ah, now

Unknown Speaker  37:24  
who's heard of a company called compass? One or two? Okay. There's a company in America called compass, that in virtue of using some algorithms to trawl huge amounts of data about a huge amounts of criminal offenders over several decades looking at ever know that. Because it's America. On the one hand, they're bad at looking at race. But then they have all sorts of cutting ways to work out with the racism of a person, even though they don't actually say that they're looking for risks was America, and this is how we're rolling now. What they're aware of are things like somebody's age, somebody's level of education, where they grew up the neighborhoods that they're from the schools, they went to these sorts of things. And anyone who knows anything about America knows that the neighborhoods where somebody grows up and the school they went to is pretty much going to tell you that person's race right straight away. Okay. Now what compass says it will do? Compass said, Look, we will predict the likelihood of an offender's reoffending by trolling huge amount of statistical data much more accurately, then can any human magistrate after a couple of whiskeys at lunch, like were much much better at this. So you should pay for our reoffend the detective Miss Mecca platform and algorithms will do a much better and hence their job than you will. Because of course, we don't want to make mistakes about sentencing that will be terribly unfair. So here's a way to ensure the complete objectivity of the entire the entire system. And it's a way of getting humans out of the loop. Right and you the machine couldn't possibly be biased. You know, the machine has no values here. The machine can't be racist. The machine can prefer one person over another. It's just looking at the pure math, and the math doesn't lie.

Unknown Speaker  39:43  
Does anyone know what happened? Okay. Firstly, it was no better than a person for the most part

Unknown Speaker  39:59  
seven The It was no better than not just magistrates, but random people. Right, who chose to look at an individual person's criminal background and their own lives and something and I guess, and that's without reinforcement. So people, individual humans were given historical cases and then told that they were right or wrong. And then they got much better at predicting recidivism than the algorithm. What's more than that, what's worse than this is that although the machine the algorithm made and and this an equal percentage of false predictions irrespectively of race, there's a difference between a false positive and false negative.

Unknown Speaker  40:45  
And for whites offenders.

Unknown Speaker  40:51  
It was the false negative rate that was high. So I said these people wouldn't offend them, a certain percentage of them went off and did it. And then for the black offenders, there was a greater number of those equally high number of false negative negatives, that predictions that this person would offend, so they shouldn't be sentenced and okay, but then they go on and they don't offend again. But no one thinks that sending someone to prison stops reoffending. If anything, the greatest prediction of reoffending is the fact that you've sent somebody to prison. If any, if anyone hears knows anyone who's been to prison, then even for just two weeks, like how about knowing this incredible criminal stuff that you don't even notice there before? It's like a training school for criminals? So, really, where do you go to learn about crime where tell me where all the criminals are, and I'll go there, it's not that surprising. Okay. Now, the big concern has been that the algorithms are not all that accurate. And one of the concerns that have been turning up in the media here in Australia, is that we might be outsourcing these sorts of decisions, to algorithms that will then go on to make mistakes. So now the big research drive is to tweak and perfect various algorithms that will predict recidivism successfully in such a way that the mistakes in predictions will be mitigated or massively minimized. So that the predictions made by the algorithms will be much more accurate across the board than any prediction of recidivism, of reoffending, that could be made by a person. Now, for all of you here, and for those of you watching remotely. Who's whose thinks that the right thing to do morally here is to continue to try and optimize the success rate of the algorithms in question so as to minimize errors across the road, either false positives or false questions or false negatives?

Unknown Speaker  43:08  
Who thinks it's the wrong thing to do? At the back, why do you think it's the wrong thing to do?

Unknown Speaker  43:20  
Machine is good. But it doesn't it can just do so based on relationships. For example, it would really confuse like, right, even if

Unknown Speaker  43:36  
it could really abuse what's our addiction, it could just be doing. So the the concern for those online is that even though the presumed predictions might be better, but they might be predicated on something that socially like, just for example, racial discrimination, that could be a structural issue that we might address perhaps more cleverly than simply in terms of sentencing. Yeah, okay. What were the other kinds of who has a concern that is different to this Yes.

Unknown Speaker  44:41  
Yeah, that's such a fantastic response. But so for those online again, the worry here is that look, false positives and false negatives shouldn't be treated on par in the first place. Letting a guilty person go free is not nearly as bad as sending an innocent person off to prison. That's the some side on To whom this is attributed that, you know, let 1000 guilty go on punished less, you know, one is the person is punished or something like this. So your your consent he'll be more the the tweaking should be more in favor of minimizing false positives and it is with false negatives. Okay. Are the others Yes? Similar

Unknown Speaker  45:41  
to the New Statesman of the Bible and all of a sudden it affects your search equally, like just haven't been used as sustainable

Unknown Speaker  46:09  
Yeah, okay. That's that's another excellent response. So what this response is online people is that we should stop perhaps think very carefully about trying to optimize sending people to prison in the first place. Because unlike America, we're sending people to prison keeps the economy afloat. And it's a it's a for profit prison, where we might exercise some caution before we start trying to optimize incarceration and think a little bit more carefully about whether we wanting incarceration in in the first place. So I'll expand on that you're in the Netherlands, they're closing prisons, because they've run it out. They're running out of people who are breaking laws. A part of this is just by making everything legal. I'm thinking Tada. But, but, but but also, because you don't have the the terrible inequality that you have in somewhere like America, where you have a whole bunch of people are going to have to sell their house because they need chemotherapy, eventually, they're going to rob you. That's not weird. So we might want to think more carefully about whether whether or not we want to be incarcerating people in the first place. And spending our time optimizing an algorithm to assist us in the process of incarceration might be distracting us from that important discussion. Is that Is that a good summary? All right, excellent. What other concerns would you have? Yes, yeah.

Unknown Speaker  47:36  
So I think someone raised the point of like structural issues, with like, say, maybe the algorithm looks at race and says, This person in this race, they're more likely to reoffend. And I think if we look through that lens, it kind of highlights a problem with AI itself, which is you can analyze how accurate decisions are. But oftentimes, it's a black box, and you can't tell how the decisions are being made. Yes, some really bad structural issues where it does mean that race is an indicator of crime, but it's because of the structural issues of say, like poverty or something. So it appears to be more accurate. But it's not actually like its decision making process in Rome, like any kind of inherently and you can't analyze that and figure out why it's,

Unknown Speaker  48:20  
yeah, so yeah, exactly. So to repeat for those online, the algorithm will not be answerable for the reasons that it has for making the decisions that it made. And this is what I think is one of the most important issues that is being lost here in this debate. If we're talking about depriving somebody of their freedom, then even if a human is going to be less accurate than a machine, possibly, in principle, there is one thing that a human can do. And that is to provide a response when we ask them for why it is that they did it. And we should, I think, be very careful. Because again, it's something that doesn't have the best track record in our species, of enabling anyone in any kind of position of power be at a magistrate or anybody else, to act in a way that affects the freedom of others. That at the same time, excuses them for taking from taking any personal responsibility of that decision at all. Like I'm sorry, I'm going to send you to prison now for 10 years or something. Why but I don't really know why. Right. But the machine here says I have to, and we've run through all of these field test is on their way more accurate than I do. So. You know, I you know, my hands are tied. There's not much I can do here. You know, machine says you have to go to jail. Now that makes us angry enough, when all it is happening is some weird enrollment problem you have, and you ring someone at nucleus. And it was like, I don't really know why but it says you haven't met something and I can't really override it, I guess try again next year. But imagine if that's sending you to prison. And now imagine if we're living in a world where people sort of think that that's okay. Or at least some people do, presumably not the people being sent to prison. It's not obvious that that world is not much worse than the world we live in now, where magistrates have very fallible, but they're answerable. And if they're systematically or egregiously fallible, then they get in trouble from other magistrates. I think that's that, I think that is hitting the nail on the head. Okay, it's five two, is that okay, great. Last question before we get out of here on time for the first time in a month.

Unknown Speaker  50:56  
Questions. So I feel like also, it's kind of like a practical result of people trading correlation or causation, we assume that the system reason out, it's resulted in accuracy. But it's also you can you can train an algorithm to be like, Oh, look at ice creams and look at violent crimes. And it's like more ice cream sale, five minutes away, it could just be the weather that attributes the best person. The second thing is regarding like, issues of this system of like, training, like the algorithm to once a second, but people will be right. If we're constantly constantly training it. There's also the power like self weight, where we say we like it increases the virus is really amplifies what buyers say this person has reinvented. Alright. And then we sent more police to those neighborhoods. But because of that the police catch more offenders in those neighborhoods, just by nature, and then certifies

Unknown Speaker  52:09  
Yeah, you're absolutely right. The concern here is that by continuing to optimize these algorithms, what we might be doing is just reinforcing a bias. And the analogy is, if you find, if you think a neighborhood has a high crime rates, you send more police in there, they'll have more police there to catch more people commit kind of committing more crimes. So then that, of course, is going to reflect an even higher figures in the crime rate for that neighborhood, which remain even more police going in there. Until you have you know, somewhere, like some cities in the world where you just have three police officers standing on on every street corner. And that's not obviously the optimal solution. Like if that's what you needed. If you needed to put three people with guns on every street corner, to bring the crime rate down. The number of terrible decisions, socially you made to get to that point is is just astonishing. You you might have tried harder, is what we're suggesting here, and you and we can we might the dangerous, we might achieve the same thing algorithmically. If we end up with a slogan if we end up with machines, sending people deciding when we should and shouldn't send people to prison. Perhaps we made a lot of terrible decisions on the way to that point where we think we need to start automating the process. And we could have tried a little bit harder. Okay, thanks, everyone. For those who have questions, please send me outside. Let's for the first time of the month, they just said that the poor lecture for the next course in here on time is the least we can do. We'll see if we can keep doing it. Alright, thanks so much. Everyone online,

Unknown Speaker  53:56  
thank you as well.

Transcribed by https://otter.ai
