Unknown Speaker  0:04  
Hi everyone we're still setting up so bear with me

Unknown Speaker  0:12  
I'm not

Unknown Speaker  0:15  
if I showed you how everything works yeah but thank you yeah there's um it's like the the number the amount of time you have multiplied by the number of people involved is how long it will take

Unknown Speaker  0:35  
you being able to find

Unknown Speaker  0:50  
did I bring

Unknown Speaker  0:53  
I did I'm good

Unknown Speaker  1:27  
I was really hoping they wouldn't be a lecture it'd be four hours so I could come in 15 minutes early but clearly that's not the case

Unknown Speaker  1:35  
only there really is

Unknown Speaker  2:07  
wow Ah great someone sent me messages already how excellently convenient

Unknown Speaker  2:14  
counting on getting

Unknown Speaker  2:20  
back to that Okay Are any of our tutors here I need your glasses badly

Unknown Speaker  3:43  
no Not at the moment

Unknown Speaker  3:45  
I thought I said you undecided ways to make money

Unknown Speaker  4:01  
do

Unknown Speaker  4:04  
not connected I know you're not connected but you will be up whoever designed the new iPhone interface so the iPhone long thing is where the turn my phone off thing used to be talk about interface design fail all right applications

Unknown Speaker  6:00  
morning Economics okay

Unknown Speaker  6:53  
testing microphone people on Zoom, can you hear me all right good, good. Okay. People behind the microphone they can hear you too. I mean it's it's cardioid it's not Omni but they'll you'll hear a little bit of you. I'm going to repeat everything that I say except that this bit don't mean a lot I'll just use the Blackboard and get chalk everywhere because that's how education

Unknown Speaker  7:25  
okay

Unknown Speaker  7:29  
you are working you are working what time is it? Oh, that's quick. It's only 10 past is going to excel at least the quarter past the day anyway. What do so it's a vote what do you think I should do? So should I set it now the camera up to do a separate recording to be on the safe side. But should we live life on the edge and then if the Wi Fi drops out again like it has, you know 35 times this week? They'll be no lecture recorded. live on the edge set up the camera Oh, if that works? Sure. Yes, it is but it often doesn't work. I know very well. Yeah, Echo Echo is sketchy. Echoes very sketchy. Oh, no, it absolutely doesn't matter. Because I changed it so it's recording on my harddrive. We're good. I don't even worry with the on the camera. Okay. Right, right. boring stuff. I'm sure you've seen it already. But if anything happens at all

Unknown Speaker  8:51  
that's my email address for you people over here so you can see it as well. No, you know how it is. I run out of Blackboard. Don't stop. All right. Hi, everyone. If you're not here for 4920 You're in the wrong legislature, which usually doesn't happen by the time you're in third and fourth year. But one of the greatest tricks you can play if you get into academia is with the very first really big first few lectures are the ones where there are often 500 people just walk in get everything set up. Wait until it's too late for anyone to get anywhere else. And then then just make up some subject. I say all right, everyone, if you're not here for economics, one on one, you're in the wrong place. And there was like oh my god because I don't know no way around it. I know that that really breaks the ice. But I can't do that with you. I'm Sam I'm so glad and happy to be here. I'm delighted that so many of you are here as well. This chocolate going around if you've arrived late and you've not yet acquired or access the chocolate, you're being gypped so find the people with the chocolate wave it around so the chocolate makes an appearance for those of you watching online. I'm So sorry, I can't give you chocolate online and when the when the when the I'll look at you intermittently. Actually ta if I make you post and then if somebody says something like help gonna yes all right where where are you?

Unknown Speaker  10:21  
I haven't done yet but it's really it's

Unknown Speaker  10:29  
been excellent everyone I'm just gonna make on a co host so that you know when it all goes wrong it'll it'll be transferred off is absolutely not jet Labrador Um Where is it the link is only web CMS page nationally this is good on interpretive dance

Unknown Speaker  11:19  
you're in my name is Tip day, it's what I see at the end. That's probably not just Okay.

Unknown Speaker  11:30  
Make co host right co host you are co host. All right now you can see things and if something in particular, just put your hand up and Okay, so I can't trick you into believing that you're in the wrong lecture theater. But I do want to start with a story that's relevant. It's a story about two friends of mine, Alex and Erica, their brother and sister Erica did her undergraduate degree in computer science here. And as we speak, she has offered Columbia University in New York talking about something to do with computers. I didn't Aleksey his master's here. And he is as we speak, somewhat less glamorous. He's in Auckland, but he's working on AI and stuff. He's just working remotely for this company in America. And it's all great fun. Now, many, many years ago, all three of us were living in Europe at the time. And it was Alex's birthday. And Erica works on big advertising firm. And she said for Alex's birthday, we're going to go out to this huge party down on Liverpool Street. So we went and advertising people have so much money and no taste. And it was incredible. This party was sort of in the the atrium area of this huge cluster of skyscrapers. So we were inside, but the inner ceiling was like 25 floors up. And it was like something like a die hard, you know, the pop was hanging everywhere. And there were at least three or 400 people in this party. And the party looked amazing, but there wasn't much going on. So before we before too long, we left at a reasonable hour. I was walking home with a whole bunch of Alex's housemates friends. One of them I just met that night. And we got chatting and I was like so you know, what are you up to for the rest of the summer? And so, and she said what I thought was, I'm going to Portugal to study ancient poetry. Now, that's what I heard. And what I understood and believed what she actually said, We will definitely music inside what she actually said was, I'm going to Portugal to study ancient pottery. And this began one of the strangest conversations that I have ever had in my life. I'm saying things like, wow, you must be really smart. Like that's, that's hon. Do you? Do you do you read Latin? It looks at me oddly says, no, no, no, we're not sort of concentrating on on the tactility. You know, and I tickle. Okay, right. You speak ancient Greek. Aramaic, perhaps? She's hieroglyphs cube or anything? She's giving me really strange looks. I say no, no, no, I just like to, to work with my hands. With us. You're walking along. And I think she's crazy. And she thinks I'm crazy. And the conversation goes on and on and on until it makes no sense at all. And you'll find that to say, hey, stop, stop, stop, stop.

Unknown Speaker  14:33  
Say it again. Why are you going to Portugal? And then she says hace que it's the Oh, I'm so sorry. I heard you. I misheard you. I thought you said ancient poetry. And she said I loved music. You must have thought I was crazy. And I said no. Well, actually, yes. But apart from that. We're good. Now. You know, it was a solid like four blocks and I'm trying to be you're trying to be incredibly polite and not to say what you're saying does not make any sense until it gets impossible to avoid. I think part of this was on me out of it was my fault. Because even though I didn't understand what he was talking about, I was being really polite, right? And I wasn't just saying, hey, stop, you're not making any sense. What do you want about I just kept politely trying to steer the conversation away from that pathologically mad over to something that I understood. Right? My advice, and my sincerest request to all of you is just don't be me. Okay, if I'm saying anything in here, that you don't understand at all, that's my fault. Right? If you don't tell me that you don't understand. That's your fault. And that goes for you people online as well. So please, and I made this completely sincerely, to put your hand up straight away. If you haven't understood anything that I've said, I'm going to ask you all the time. And I'm going to most most students are not very good at poker. If I'm being really unclear, I tend to look up at a lecture theater, and there's a couple of 100 people going, and that kind of gives it away, right? So don't take up poker in a hurry. I'll keep my eye on this. I'll try and notice, but I promise you, if you don't understand what I'm saying, at least 60% of the people in the room don't understand, as well. So that's our deal. You got to interrupt me with all and any questions at all, especially ones along the lines of say what you want. Is that a deal? Now the deal? Is lots of happy nodding. Okay. Excellent. Now, on topic, speaking of the ancient world, here's something that's been working for 1000s of years is this idea, the idea is whether

Unknown Speaker  16:56  
we're going to play a game of make believe, okay, now this game of make believe has been played from two and a half, if not 3000 years ago, all the way up till now. Okay. So here's an input to all of you, in this course, you are the device that's being optimized, right? You your your minds. So here's the first piece of input.

Unknown Speaker  17:20  
All humans

Unknown Speaker  17:28  
that's the input. And here's the game of make believe we're going to target with the value true. In other words, we're just going to pretend that this claim is true. It doesn't matter whether it is actually true or actually false. It doesn't matter whether or not you really believe that it is true, or if you do not, because what we're going to do is give it a placeholder truth value of true, like, it probably is true, but that's beside the point, right? We're just going to pretend that this is true. Or as we say, in the business, we're going to assume that it is true. Everyone with me so far, it shouldn't be too, too complicated just yet. Okay. Now, here's another claim. Slightly less uncontroversial. But again, we're going to assume that it is true or target with the value true. Here's the second claim.

Unknown Speaker  18:31  
Donald Trump is human.

Unknown Speaker  18:37  
Work with me? Come on, work with me. Right? Here's our second input. Donald Trump is human. Stop playing fortnight and suburban pay attention. put you behind the camera started out with like, I was pretty good. It was up there. All right. Donald Trump is here. So now we have these two bits of input these two claims. The first one is that all humans are mortal. So here we go. We've got the collection of all of the mortal things. And in here we have all of the humans should probably all humans are mortal. There's not a single human outside of the mortal things. We've said that Donald Trump is human. So Donald Trump, the half here is Donald Trump is one of the humans now. I want you all of you now fearlessly to execute all of your faculties of reason. All of your logical progress, monitoring you to engage in a in an exercise or a piece of internal error free computation. If both of these claims are true, if they are and we assuming that they are, what follows as the upward? What if I say to humanity students, what may we conclude on the basis of our assumptions as a different group choose different languages? Yes. Donald Trump is model Absolutely. So we say, in the world of logic, we say, therefore, but you don't write that down, you say, therefore, all the time you will be writing therefore, down all the time, you can either move three little dots in a triangle, which is stupid and annoying, or you can draw a line which is fantastic. The line means therefore, that was much easier than writing the word therefore, I can assure you, and you can say, therefore, Donald Trump. Or Barack, as a slightly adjusted example, from ancient Greece used to be Socrates. But now instead of Socrates, we've got Donald Trump instead of be lucky. Now, notice that each one of these things is a claim. A claim is the sort of thing that can be either true or false. They're fancy ones that can't be false, like either it is raining or it is not raining. But they're the sorts of things that can have truth values. That's what distinguishes a claim from other types of speech acts, even though the other types are well formed. Here are some examples of some other types. Think of requests? Excuse me, may I have the salt? That is the request, it is not a claim? Excuse me? May I have the salt is neither true nor false. It doesn't make any sense to say of Excuse me, may I have the salt that it has a truth value at all claims, a very special types of things, that the types of linguistic items that can have truth values are the ones that can't have things like commands, like study hard, right? That's the command but study hard isn't true. Neither is it false. Again, it's a category error to say of study hard that it's true or false. You with me so far, to anyone not with, Okay, now, if I'm being really fancy pants to say actually, it's not these individual claims that matter. It's the abstract objects that they express, and they're called propositions, that doesn't matter for this course. It's like this thing here, isn't really the number three. The number three is this abstract object about which we know all sorts of things that we represented the Tyvek script like this, or you can use roman notation if you're not, but whatever. We don't need to go into abstract objects in this course. Maybe not till the very final week. So here, we're just going to treat propositions and claims as all as the same thing. The only point I'm making here is that this is a list of claims or statements or propositions, can use all three of those terms interchangeably. Claim statements propositions. Now, any list of propositions. The last one of which comes after the word therefore, is called a wallet. Who knows?

Unknown Speaker  23:20  
An argument, an argument is a list of claims or statements, the last one of which comes after the word therefore, who's heard this phrase in English? Let's assume this for the sake of argument, who's heard that phrase? Well, now you know where it comes from? Originally, what it means, strictly speaking is, let's assume the truth of the premises for the sake of testing the argument for validity. What are the premises? All of the claims or statements that come before? The word therefore, what's the conclusion? It's the claim that comes after the word. Therefore, what is it for an argument to be valid? It's for it to be impossible for all of its premises to be true, and its conclusion false. This is a valid argument. If these two are true, this one's got to be true, you already know how to do this. We're just learning a whole bunch of attendant jargon. Now, these are very special types of arguments, the deductive arguments, the types of arguments on the back of which computer processes are built. Computational architecture emerged from people like well, Neumann and Turing having way too much time on their hands on worrying about what happens with this when you really, really, really, really, really, really think about it. And now, you know, we have iPhones, that is pretty much basically what happened. Okay. Now, in general, an argument is a claim for which we give reasons. Not all of them are deductively valid like this. This is the simplest possible case. That's why it's been used to introduce the notion of an argument for nearly 3000 years old lot of arguments that represent are not deductively, airtight like this at all. Rather, we have some claim or statement that we believe to be true. And we're trying to convince other people that it's true as well. And we might not have a deductively airtight argument to present to them. But we have a whole bunch of reasons that are supposed to make the conclusion of that argument. plausible, or at least not obviously, false. The theoretical recent research, not obviously false isn't HD, right? It's hot stuff. Now, an ethical argument, but finding an ethical argument is an argument for which the conclusion is a moral judgment. An ethical argument is an argument in which the conclusion is a moral judgment. What is a moral judgment machine practice?

Unknown Speaker  25:56  
an ethic login. That's how I steal arguments when I call it still is

Unknown Speaker  26:09  
an arguments

Unknown Speaker  26:14  
were they the version?

Unknown Speaker  26:25  
is a moral judgment moral judgments or moral statements? The same thing? So what are the ethical arguments? ethical arguments are basically some sort of moral or ethical statement about the way things are morally speaking, or what we should or shouldn't do, morally speaking. And everything else in the argument are reasons to believe that moral judgment, that ethical judgment, that moral statements or ethical statement, this is what moral arguments are, they're like any other arguments, a bunch of reasons for what it is you should believe some statement, except the target statement in question. That is, the conclusion of the argument is a moral claim. That's what moral arguments are. That is what ethical arguments are, I'm going to use the words moral and ethical, completely interchangeably. Right? Whatever, whatever. I feel like I mean, the same thing. And if they don't, someone will tell you. Okay. Now, are there any questions at all? About anything that I've said so far? Anything anything that is? Yes? What's a moral judgment? Burning kittens is wrong. That's an example. But burning kittens is wrong. Burning kittens is morally good. That's also a moral judgment. It looks like they can't both be true. Yes. I said Jason is part in certain anything, you know, the burning people is wrong. There. Most of us think of it, it goes on all the time. Now, it's perfectly okay to burn this person there are which like, no matter what they'd see weighs more than a duck. You know, these are moral statements. Am I supposed to have sorry, for those who haven't seen Monty Python? It's like, What's he talking about? But for those who have injured, okay. This is what moral arguments are. For the next two weeks. What we're going to be looking at different theories. The how it is that we can know that a moral arguments, reasons for believing a moral statement is a good one or a bad one. Like how we this this one's easy to judge wherever you're using you. This one's easy to judge because it's suitably simple. But what sort of reasons are the right sorts of reasons to be reasons for moral or ethical claims? People don't agree on this. There is no consensus on this today. There are three main competing theories than a whole bunch of other less popular ones, we're just going to look at the three most popular ones because they sort of cannon you need to know about the most popular ones first. The first one that we're going to look at today, is utilitarianism. And then we're going to see how it is how it is that utilitarianism connects directly to a lot of issues in the ethics and morality of artificial intelligence. And that's just today. And we're going to end in plenty of time for all of you to ask a whole bunch of questions that I know you'd have about assessments and the structures of the course and all of the boring stuff that you can't do anything about anyway, but I remember being a student and you really want to know But that's for like the last half hour or something, we've got plenty of time today. Next week, we're going to look at Kantian ethics, known also as de ontology or deontological. Ethics, as well as virtue ethics. That's next. We'll have more time next week. We're faffing about, and we'll get straight onto the theories. Now, what was I about to say, Wow, that's gone. Something something something, something? Yes.

Unknown Speaker  30:31  
A good analogy for the world of computation, before we get into the meat and bones of it, is that utilitarianism can be thought of as a theory of ethical justification that relies essentially on external empirical data. This is just an analogy, we'll see why it holds in a moment. Kantian ethics are de ontology that used interchangeably can be thought of very usefully as a theory of ethical justification, that believes that process of justification to be able to be undertaken via pure computation without any need for empirical evidence at all. Virtue ethics, doesn't matter what that is yet, we're gonna do it's more like machine learning, but we're going to get there. It's more like training data sets. I think it's the best analogy. But that's for next week. This week. We're just what time is it? Now? I have plenty. If we get to one o'clock, and I haven't said let's go for a break. Can you remind me because I really want to make sure you won't get a break. Okay. This week, we're going to be looking at utilitarianism, which is a variety of consequentialism. That was a bit of an overview of this week. And next week again, are there any questions so far? Yes.

Unknown Speaker  31:58  
Save all the sorry. You just gave those like one sentence analogies. Yes. Can I? So for that, I'm going to remember to repeat this for the people that have, because you just kind of go over the analogies again, yes, I can. We can think of utilitarianism as a theory of ethical justification. And that is just reasons to believe the conclusions of an ethical and moral argument that proceeds on the basis of empirical evidence, getting empirical evidence or empirical data is a necessary condition for ethical justification according to the utilitarians makes sense so far? Okay. Kantian ethics is on the other the other hand, like what Nate, I do not examine the external world. Instead, I lie here, my boss for three weeks and come up with pure axioms of moral reasoning. And I present to you, you know, irrefutable reasons for why it is that burning kittens is wrong. And I don't need to go and examine one single burning kitchen. To do this. It's all a priori reasoning. As we say, in philosophy, it's just pure computation, without any need for external data to take part in the processes of reasoning. So far, so good, good, good, good. And virtue ethics is more like data sets and machine learning. That's it, then the analogies, I'm going to return. And we're going to do this again, slowly and slowly and slowly. And the point of these lectures is to get all of you in a position where you can engage with the readings much more deeply and more thoroughly than you might be able to do otherwise. And then what you should do before the tutorials next week, is to arm yourselves with pages and pages of questions for your tutors, because your job is to give your tutors so competent and the hardest time possible, right? I really want to hear from all of your tutors that are God, these bloody students have been there for another half hour after the shoot, they wouldn't stop asking questions that was so hard, I can't help you keep saying I don't know, I'll come back next week. And that's great. That's that's you're getting your money's worth. And it's definitely the tutors ending their case, or your tutors, and we're going to have meetings every week for hours and stuff and go over everything. It'll be fine. It'll be minimal, or No. Any other questions before we launch headfirst into a long definition of explanation of utilitarianism, and after the break, we'll look at its connections to concerns and the morality of artificial intelligence. And then I think we'll have a solid half hour 25 minutes to discuss, you know, attended details about the structure of the course. Yes.

Unknown Speaker  34:43  
I don't know if this is a real question, but it's a question from the chat. Why do you keep referencing the workplace the question in the chat, not even sure that you are referencing it at all why and if not, I

Unknown Speaker  34:57  
all the good places, probably some TV show or something. Am I Why am I Oh? Why am I not? Apparently? Oh, right. Oh, good place. I'm like, wait, heaven isn't gonna be really polite. We're getting fired or is this not? I've never seen the good place. It's so introduction and good places actually. It's all about ethics. It's like a comedy but it's all about like your ethics. Yeah, I if you knew what I actually wasted my time watching online like Oh, eight analog to digital converters as a new chip, you know, I don't have time. But thank you for the tip. If I do, I'll leave a leap on in there. I don't I banish televisions from my house because I know if I've watched two episodes of anything, no matter how stupid, I'm hooked, it's too late. Well, housemate years ago, when I was a student had me hooked on every soap opera on TV. I hate her so much wasted so much of my time. Like what's going to happen to Felicity now? All of them anyway, something utilitarianism. Okay, any other questions? Yes.

Unknown Speaker  36:14  
Know the goal of research ethics is not to unite all of the views. The goal of research ethics is to arrive at the truth. And the truth is not a popularity contest. I can assure you of this much, especially with morality. Wait until we get into the meta ethics stuff. We start when our moral judgments really propositions, we're just expressing our desires. Yeah, but that's for later, that's for later, we'll get there. That's a lot of fun. That's when you're already exhausted. All right. If you've got a hand up and I don't see you just flail the lightnings, halogen or whatever, whatever they are, it's policy. Okay, excellent, and to get your prescriptions. Now, utilitarianism is a variety of consequentialism. consequentialism is a theory of ethics, but not just any type of ethics, normative ethics for the next two weeks, we're only going to be talking about normative ethics, as opposed to meta ethics doesn't matter what method ethics is yet. What's normative ethics. Normative ethics are just theories of how it is that we can know that some ethical judgment is the right one or not. And all ethical judgments or normative claims or ethical claims are normative claims. Normative claims are ones that tell us what we should or shouldn't do. They issue instructions of some sort, as opposed to descriptive claims like grass is green or snow is white. So instruction there. But if I say Thou shalt not kill, that's a normative claim. We also say it's a prescriptive play. Okay. Now, how does consecrated Well, here's a great example, before we get into consequentialism. There is this really famous list of normative moral claims. I'll give you a hint. There are exactly it's been around for 1000s of years. Okay? There are exactly 10 of them. Yes, the 10 commandments. Sounds amazing. Somebody was like, I've got no idea. But what really, okay, the 10 commandments. Here's the note as each of the 10 commandments, is a moral judgment, it's a moral statement. That's about what you should and shouldn't do, morally speaking, not just for giggles, that's supposed to be the right thing to do. And frankly, you know, even if you're not even vaguely religious, if you were stranded on a desert island with, you know, 400 people, and you had to restart society, I think if you started with the 10 commandments, it's unlikely to go terribly wrong, at least for the first couple of weeks. It's not too bad. As far as far as moral constructions go, it's short, it's to the point, and you can go into war about interpretations of it can be plenty of time. But all of history, future history waiting. Now, notice this remarkable thing, about 10 commandments. It's not actually one of the commandments, but it is a fact about right. What they were at the time was a list of rules for how it is that one Israelite was to treat another Israelite. If you weren't an Israelite, you were fair game. Right? That didn't apply to anyone else at all. It was just for members of the tribe of Israel. If you weren't part of that tribe, can pretty much do whatever you want them to that person. And they were certainly not down under the umbrella of the 10 commandments. So there's something to think about here. And it's terribly important. There's something on which consequentialism utilitarianism has made great, great bounds and I haven't said what consequentialism is yet we're getting there. It's one thing to argue about what the moral norms should be. That is the rules that govern behavior morally. And then there is a separate conversation to be had about to whom it is those rules rules apply. For example, should they apply to members outside of my family?

Unknown Speaker  40:09  
Should they apply to people outside of my tribe? Should they apply to people outside of my ethnic group? Should they apply to people outside of my nationality or country? Should they apply to people irrespectively of their gender or sexuality? Should they apply to people irrespectively of their age, these things are not obvious. Even today, there are all sorts of things that we can avail ourselves of simply by being here in Australia. But if you're under the age of 18, you can't. You're remarkably in Australia to this day, if you are subjected to custody proceedings by your parents, right? You do not have the right. As a child, I think it's under 16. To present yourself and your own testimony directly to the magistrate who will be deciding your custody arrangements, you can have a lawyer or a legal representative go there on your behalf, you do have that right. But you have no right to present yourself there. So you can be 15. And your parents can both be not screaming at each other. And you can't actually go and speak to the magistrate at all, you have no right to do that. Whereas if something was like that was happening to any of you now, you absolutely would have that. Right. Right. So age is a fantastic example of how it is that we think that certain moral conditions apply, and then they don't, but as a kid, it was normal for teachers at school to beat the living hell out of children was ticks. And that to me, it doesn't happen anymore. Thank goodness. Similarly, one of the biggest narratives in the environmental debate at the moment, is whether or not we do have something like moral obligations to future people, people who don't exist yet, should people who will exist in the future, perhaps after we have died still fall within the remit of our moral obligations is one of Greta Thunberg. Great points. It's like, look, it's not just about us kids are alive now. It's 100 years from now all the people who are going to be alive, then we have a moral obligation towards them, as well. Of course we do. According to greater other people like hell, no, we don't. All that matters is you know, my bottom lap, these sorts of things. So we're going to be talking about moral rules here. Keep strongly in the back of your mind, the people to whom those moral rules apply, do they also apply to non human agents? Do they apply to animals, and so on? Good. So what is consequentialism? Then consequentialism is a theory of ethics that says, well, there's all this talk about the good and the bad. And there's been all of this, gesturing towards these highly abstract, more principles and everything. And hundreds, if not 1000s of years, people have gone around being terribly, terribly cruel to one another, justifying it on the basis of them actually doing the right thing. But if you're being cruel to somebody and causing a huge amount of suffering, you can't possibly be doing the right thing. Because causing suffering just is the wrong thing. causing suffering is the wrong thing. Our morals, our actions, according to consequentialist, do not have any intrinsic or inherent moral value themselves at all. The things that do have moral values are the consequences of our actions, the things in which they result, it is the consequences that matter. So for consequentialist ethics, it is the results of our behavior that give our actions moral value. So actions have moral value, but only derivatively are only secondarily as a function of the consequences that they cause or that they bring about. In other words,

Unknown Speaker  44:25  
not horribly weird so far. Okay, good. Now, there are as many different varieties of consequentialism as there are philosophers looking for publications to make them up. There are so many versions, right? We're only going to be looking at one and a half. I'll explain why later if we have time. It's called utilitarianism. And utilitarianism is different from all the other varieties of consequentialism because it says there is only one type of consequence that we are trying to bring about ought to be morally good. And there's only one type of consequence that we're trying to avoid. If we're going to be good, and it's simple, we're trying to increase happiness, and decrease suffering. It's that simple. If you're trying to if you're a utilitarian, if, if you are a utilitarian, then you think if you're trying to adjudicate or decide between action A and action b, okay, and all things being equal action, a results in more happiness and less suffering, then does action b, then action A is the right thing to do in that context. And that's it, there is nothing else to consider, other than the maximization of happiness and the alleviation of suffering. Once more, these rules according this, this sort of Axiom schema, according to utilitarians, is supposed to apply to all human beings equally, not just ask them as members of a family, not just members of our tribe, not just members of our religious group, not just members of our gang use, et cetera, et cetera, et cetera, but to everyone all the time. So the umbrella for utilitarianism is incredibly why it stretches across all human agents Who's heard of Peter Singer, famous utilitarian, right? Peter singers claim along with many others is that utilitarianism, the tenants of utilitarian ism, apply also to non human agents, that is to animals. Hence, he says he's a strict vegan, and so on. And one thing that I can say about utilitarianism, and it's absolutely true, is that it has probably done more to make the world a kinder and gentler place than has done any other theory of ethics or ethical movement. I mean, usually people who are convinced of their moral superiority, like come riding in on horseback with their swords, and we've come to liberate you, and you hurt his hand to make you better people. And now, you know, all your villages are in favor, and you're all dead. And you're like, I did the righteous thing. And then you go home with everyone's goal, and you say, I'm still a really good person, and utilitarians like, stop kidding yourselves. You're just like a marauding stealing, murdering psychopath. And so is your entire culture. You just tear on the planet, ruining other people's lives, telling each other that you did the right thing? Because of some stuff you made up, right? No, look, we can't just make up for you. This is the right thing to do. Because it makes me feel like no, no, no, there is a clear decision procedure to follow. If we're trying to work out what the right thing to do, morally speaking, is. And it doesn't involve sitting there trying to just think really hard about these unfalsifiable universal moral principles from which everything else follows. No, it's essential that you check the world, you have to see what the consequences are of your actions. And hopefully, if you're not terribly removed from the nature of other people, you'll be able to make a pretty good assessment of what you think the consequences will be, where those consequences are levels of happiness and levels of suffering. On time we got all good. Okay. Now, before I launch into the next speech, are there any questions about what utilitarianism is? How it works? Okay, who thinks it's a pretty good idea?

Unknown Speaker  49:04  
thinks it's a terrible idea.

Unknown Speaker  49:08  
One, who's really not sure I'm not going to put their hand up because this is probably a trap. Smart people should be putting up now. Okay.

Unknown Speaker  49:21  
Here's

Unknown Speaker  49:24  
one problem for utilitarians. It seems like on the basis of utilitarianism, as it stands, we're going to come return to this after the break, that it might allow for all sorts of things that we could antecedently think were absolutely horrible, if those things just happen to bring about a huge amount of happiness for the majority. Let's suppose for example, if there was a small All minority of people. And if by enslaving all of them and making their lives, maybe everyone else's hates them. I mean, that's only happened 1000s of times before in our history, all these people, we hate them, they must die. That's pretty much the history of the war. And they've got money and resources, which is a terrible place. And if we're just really horrible, this small group of people, like everyone else would be super, super happy. And it looks like utilitarianism should should just say, Yeah, young Go Go right ahead. It's the right thing to do. And there are all sorts of interesting thought experiments along the lines of one of the ones in the reading that hundreds I did. I'm a sheriff in a small town. And I know that this man over here is innocent. But everyone in the town believes that he's guilty. And it's like some horrible serial child murderer or something like that. And the child, everyone in town is convinced that he's guilty. And there's no way I can convince them otherwise, who knows why he just can't so thought experiment and make it. So surely the right thing to do in that situation. If if I was utilitarian would be to imprison and perhaps execute, whatever the innocent the innocent person, now sort of naive utilitarian responses to say, oh, but people wouldn't really be made genuinely happy by extraordinary amounts of suffering of sort of like otherwise innocent people, which is a remarkable thing to say, if you have even less cursory knowledge of human history. When you take, for example, the Holocaust, the Holocaust was an Adolf Hitler and three crazy friends and everyone else going on, what are we going to do? There was millions and millions and millions and millions of Europeans going Yeah, and the Nazis are kind of gonna kill all the Jews great. And then when they got caught, they went oh, sorry, is a hardware cost. That's the only way if it happens with millions of people were behind it. People can do horrible things, and be made terribly happy about it at the time, until they sort of sprung and bought. It seems like the misunderstanding of human nature here can be a bit fake, we're going to hear the utilitarian response will often be Oh, yeah, but they've got all sorts of false beliefs about what really makes them happy. But then you can't argue with that. It's a bit like the false consciousness belief in the Marxist revolutionary. There's just no way out. From this, this, this, this argument, anyone who says that they don't want to live in the system is actually living under a false consciousness, and they just need to be re educated. And then how do you know when they no longer living under a false consciousness, but when they agree with everything that you're saying, which is slightly biased. There are 1001 really naive belief systems that work like this, that was the normative philosophy. Sophisticated, utilitarians have recognized this sort of naive utilitarianism might not actually be the way forward as a difference between act utilitarianism that we've been discussing. So, so far, and what we call rule utilitarianism. Now, what's the difference with utilitarianism that we've been discussing so far, the right thing to do is what ever action it is that will maximize utility, which is the greatest balance of happiness and suffering. And that's it. Now rule, utilitarians like to generalize a bit more than say, looking for the right thing to do is the thing that is an instance of some rule about how we should behave. And that rule is one for which the General General following over would tend to maximize utility along the long term. Now, the response from the act, utilitarians at this stage is to say, No, you're giving him too much. Now we're getting into like, like highly abstract moral rules again,

Unknown Speaker  53:55  
all that matters is the consequences for utility measurements as a function of our actions in any particular instance. And what utilitarianism is telling us is, what the right thing to do would actually be. What it's not telling us is how it is that we will be certain that this action would, in fact, result in the maximization of utility. And in in situations where you've got a whole bunch of people who have false beliefs about the guilt of some person or so on, then the actual this is a really tricky situation. And honest, utilitarian to say, you know, in a situation like this, gosh, that's terrible. I don't really know what the right thing to do is there and facts of the matter about levels of happiness versus levels of suffering, are an empirical matter. They're an empirical matter is that we have to go and examine the wall and there's nothing about utilitarianism that will make you certain about the levels of happiness versus suffering before you act in such in such a way But that's the beauty of utilitarianism. It makes utilitarianism compatible with the natural sciences. It made this utilitarianism, a naturalistic theory. So instead of talking about weird moral stuff out there in the ether, we can talk about happiness and suffering that are actually miserable states of a human being in the world. So we're replacing talk about the good and the bad, we talk about happiness and suffering. What do I who knows what naturalism is?

Unknown Speaker  55:32  
Go for it.

Unknown Speaker  55:34  
In terms of ethics, in terms of anything, it's a globalist view.

Unknown Speaker  55:40  
We can actually observe

Unknown Speaker  55:42  
Oh, right, good. Not quite, that's an awesome answer. It's an awesome answer. Rather, yes.

Unknown Speaker  55:50  
It's sciences.

Unknown Speaker  55:54  
So getting closer, closer, who else wants to have a go? That's so close. Did you look it up?

Unknown Speaker  56:00  
Not yet. Okay.

Unknown Speaker  56:03  
Ah, okay, then you're off the boat go for? Everything arises from the belief that everything arises from natural properties and causes? Yeah, absolutely. It's the rejection of super naturalism. Naturalism, in general, is this sort of abuse and say, Look, anything that happens happens in the natural world. There is no supernatural world out there. Right. Anything that happens in the natural world is an instance of the natural world conforming to natural laws, there are laws that govern everything in the natural world. That's not to say that we know what they are. But they're, so far, the best story that we know how to tell about the conformity of the natural world to natural laws, is the story told by the natural sciences. Therefore, according to the naturalist, if ever, there is conflict between an explanation given by the natural sciences on the one hand, and an explanation given in any other terms on the other, the explanation given in terms of the natural sciences is to be preferred every single time, every single time. That's naturalism. That's a fairly strong version of naturalism. And it's not to say that science is complete or that everything has been explained by science so far, let's just say that real explanations are given in terms of the natural sciences, that is to say they are falsifiable, possible, but goodness, falsifiable or whatever they said about 50,000 fantasy books. real explanations need to be falsifiable in light of evidence, which is the scientific method. And they need to be repeatable. And utilitarianism proposes to give us a method of assessing moral or ethical judgments that are naturalistic in just this sense. They allow us to test moral statements in virtue of the happiness and suffering that they bring about. And that is why utilitarianism is incredibly popular today, because it doesn't gesture towards strange, esoteric moral principles. Yes, it's about

Unknown Speaker  58:22  
ease into natural and suffering, yes. Have you met your happiness and suffering? That's such a great question.

Unknown Speaker  58:46  
So I'm just going to repeat your question if I made to the microphone. The question is, these utilitarians kept stepping on about measurements of happiness versus suffering. But how on earth do we measure those? Right? And this is a standard objection to utilitarianism. You should argue about this in the tutorials a great live, here's utilitarian response is okay, crazy. utilitarians will say, Yes, we can we just haven't worked it out yet, because it's really hard. And there aren't many people who believe that I had friends who read, but I think the more sensible responses say, Well, of course, we don't know how to attach discrete values to it. To happiness and suffering. We pretend we do because we like to pretend we can do everything with probability theory, because we learned lots of it in high school. And now we can pretend, look, I can publish more. Here's my mathematical system of utilitarianism. That's largely like a joke, right? But seriously, we might not be able to measure it with discrete values or assigned discrete weights to it. Um, for some often imaginary but not always, like thought experiment cases, there might just be these borderline cases and we don't know what to do. But the existence of borderline cases doesn't matter. There are borderline cases for any categorical system. Think of just the default It's between a chair and a store. There might be some, it's like an I don't know, it's kind of like nearly a stool and nearly a chair. Go back to Design School, this is horrible, right. But for the vast majority of cases, we do know for any item if it's either a chair or store out of that set, and this is the sober utilitarian speaking here. For the vast majority of cases, it is not hard to say there is more happiness and less suffering over there than there is over there. Right. And these two situations will result from my actions. So in the vast, vast majority of cases, it is not difficult at all, to assess a moral judgment in terms of utilitarian principles, even though we can't attach discrete weight or we can't treat it like weighted variables, you know, so is the assumption that was

Unknown Speaker  1:00:58  
really what happiness is.

Unknown Speaker  1:01:01  
No assumptions, not quite the most people will agree on what happiness is and what suffering is, insofar as definitions are concerned, once you go down that road, like happiness is something if and only if, and you'll never, but when it comes to just pointing it out, is that Joe calm? The guy, the film, the candy lovers in about the porn key in America, and he challenges the judge. He says, you define pornography, and the judge says, Mr. Flynn, I don't have a definition of pornography. But let me tell you something, I know when I see it. Right. And that's as good as he's gonna give. So I think what the sober utilitarian will say is that I don't have a definition of happiness for you. There's probably some incredibly complicated psychosocial, neurological definition. Who knows, right, the emotions, but I know that when I see it. Now, the problem for the utilitarian, I think something you're getting at is that what happens if two groups of people really disagree about whether or not someone is say, genuinely not suffering on the one hand, as opposed to say, causing themselves deep moral harm? You hear this I hear this argument against harm minimization things all the time, like, oh, then but what they're really doing is harming their morality. And then now when somebody says that, this kind of unfalsifiable again, right, harming the morality, then I can just say, that's actually making my morality really, really happy. You shouldn't see my morality on Saturday night. It's never been that happy, right? And now, once things get into the territory of the unfalsifiable, you're allowed to call shenanigans and say, Whoa, you just deliberately dragged this discussion over some esoteric thing about causing myself moral harm. Like, oh, I'll show you bodily harm in the carpark right now it's up to you, you can really just definitely don't, don't fall for that let somebody drive drags the argument into the realms of the unfalsifiable. They're up to something every time.

Unknown Speaker  1:03:02  
Sorry, go for it.

Unknown Speaker  1:03:09  
We've been talking about utilitarianism the whole time and just didn't bother calling it utilitarianism, nativism, and contrasting it. One of the responses to the objection along the lines of what if you just taught you these five people in here and made the whole world happy is rule utilitarianism, which says it's not just it's not the individual acts that we assess, but rather, the generalizable rules under which the acts follow. And those rules will tell us what the right axon if, in the long term following that rule would tend towards a greater maximization of utility. Okay, finally, yes.

Unknown Speaker  1:03:45  
Chain. These two

Unknown Speaker  1:03:53  
is this ship. Sorry, I just wanna make sure I didn't misinterpret that before I say, trying to

Unknown Speaker  1:04:04  
go off. Ah, so the question for those who on Zoom who might not have heard that is, if you're trying to get the empirical data, would that mean that you would be running off and doing all sorts of terrible things to people to see what brought about more or less suffering? Oh, oh, hopefully not. If you were even vaguely neurotypical you'd be able to make a pretty good life. Like GE if they were sticking hot knives underneath my toenails, you know? If they're too heavily disagree, this A disagreeing groups, you can often point to other parts in the world where certain policies have been undertaken. I think, a really fantastic example, right now where there's a big moral disagreement about moral harm and what will cause more or less harm is drug prohibition and drug criminalization. We're right now we have police officers sexually. assaulting teenage girls at music festivals, and somehow this is causing them less harm than smoking a joint. And I think that is just so insane. And and so the last people, we want to ask for advice on something like I don't know, social justice issues or or prison rehabilitation or something would be Americans, right? Why would you ask America what to do with prisons? This is crazy. Ask the Dutch whether the crime rate has been going down, the last people you want to ask is how would you be putting more and more people in prison, and the crime rate has been going up and up and up? That doesn't make you don't, don't ask the previous Australian Government about climate change protocols, you know, just don't. It's really if you want to know how to do something fine. So people who've done it before they're good at it, and ask them and usually for anything that we're doing, marriage equality was the other one people in a similar thing. But if if gay people get married, there'll be marrying trees and everything will be on fire. And we will all go to hell be terrible. And it'll be the worst thing in the world. It just look around the world. You know what happened? We have marriage equality. And all around the world long before Australia did. Gay people got married. That was it. And unless you knew some gay people got married, you probably wouldn't even know. Right? So nothing terrible was going to happen. But people were trying so hard in Australia to convince all of us that there'll be these horrible consequences. Right? And that's not true. It's obviously you could just point everywhere, but nothing horrible, you know, as as how to another great one illustrate this is we're quite worried about this as children in school uniforms. And we don't put kids in uniforms, then then then they don't know what this is. This thing is the worst thing in the world. No one else in the western world apart from UK uses school uniforms. You know what happened? Nothing. kids go to school wearing the clothes they were wearing before they got there. It's okay. You can point the consequences really easily? Yes. Oh,

Unknown Speaker  1:06:45  
somebody raised the fact that is currently

Unknown Speaker  1:06:49  
is nine paths. They tell lies. Okay. Let me find out who they are. They're losing them. All right.

Unknown Speaker  1:06:57  
All right. Let's go with have come back with a quarter past, more than happy to if anyone has questions in the in the break, come up and ask them. I will come back here at a quarter past. Okay. For those of you who had questions I didn't get to around here somewhere. I'm sorry, I will get to yes.

Unknown Speaker  1:07:17  
So I said something along the lines of naturalism.

Unknown Speaker  1:07:25  
In general. The trick is, we can't observe, say a lot of abstract things that we think are real.

Unknown Speaker  1:07:42  
And this is where naturalism gets in trouble. Say, for example, the greatest epistemic success story in the history of our species, our knowledge of all matters, geometrical and logical, mathematical. No one's ever tripped over the number three. Oh, yeah. And whatever the number three is, presumably, it can't enter into a causal relationship with us because it doesn't have spatial temporal location, which is a necessary condition on a causal relation. And a causal relation is a necessary condition on the transfer of information. And the transfer of information is a necessary condition on knowledge. So the huge explanatory gap and naturalism is how do you explain our knowledge of logical mathematical stuff? I really say admit that that is the problem. Whereas if you say to a naturalist, you haven't explained demonic possession that is like, Oh, God, there is no such thing. That's the difference. Yeah. Yeah, if we have a little bit of maybe I'll touch on that. That's for either the last week or the second last week, we're still switching those lectures around the workout who's doing well, we're organizing this one week at a time we'll be fine. Everyone will be

Unknown Speaker  1:08:54  
said yes. To questions in the chat. On the main sorry, the first one, you might have titration codes.

Unknown Speaker  1:09:21  
You will learn about both starting with the theory. Because when you look at when we look at things like ethical principles, and different companies, and so on, before we can start assessing those critically analyzing them, we need to have all the antecedent knowledge. It's like trying to connect somebody's mathematics if you don't know the algebra, you would not do it. You come away at one of the lectures I'm giving in about three or four weeks, I think three or four weeks is on ethics rushing, which is this pretense of moral concern and moral reasoning. But it just involves your company putting up a page on its websites with a bunch of really like ah, parts of it. Are they cooked up on a Friday afternoon to to keep people happy. And I'll get back if there's no actual ethical traction. So we're going to look not just at the small rules and so on a moral laws, but we're also going to look at whether or not those moral rules actually have any moral clients or any moral habits. And then we go into assess not as the contents, but how was that they then they came into being the genealogy

Unknown Speaker  1:10:30  
question whoever else. Second one once again, saying

Unknown Speaker  1:10:44  
what it means to say, to never let an argument drag into the realms of the unfalsifiable is to never let somebody take a discussion into an area where it's impossible to adjudicate between competing claims. So an example would be I say, we all know. Gay marriage will lead to people harming their own morality. And then you say, oh, no, it won't. And I say, Oh, yes, it will. And you say, all No, well, I mean, it's just how would you ever test the consequences of either his positions? That sort of argument about closing oneself and moral harm? How could you possibly arbitrate between competing views? That doesn't even mean, under under what conditions would count one's morality be assessed as true or as assessments false that isn't just questioned? And looks like you're deliberate attempts to take the argument somewhere where you can't prove your your your your opponent your oppose or false and they've done that really deliberately. Either

Unknown Speaker  1:12:20  
not even in that contrived example, because if it was if it was altered, so it was just a universe where there were only three of you in a universe where for some reason there were only three of you, but then utilitarianism would say yes, that is the right thing to

Unknown Speaker  1:12:43  
do some questions in time

Unknown Speaker  1:12:45  
over Yeah, anyone can pilot enough to get the arguments go and if people disagree that's even more Yeah, we'll come to all those shortly.

Unknown Speaker  1:12:57  
Just to check to make sure Oh, yeah, I'll spam everyone about it yeah. Thanks fine for a pleasure management that's all good

Unknown Speaker  1:13:23  
didn't really finish here there was a time before our luncheon for more details and everyone's if it's really nothing else right

Unknown Speaker  1:13:39  
say he actually would help anyone understand what's happening

Unknown Speaker  1:13:49  
that's very great. Thank you again, the readings are good. Fantastic, yes.

Unknown Speaker  1:14:16  
Yeah, there's sophisticated versions of your utilitarian like actual utilitarianism, you have to stop doing calculations on expected utility on your tree of your degree of confidence. It just gets Bayesian and game theoretical very quickly.

Unknown Speaker  1:14:48  
If the world if the world is actually genuinely chaotic, and that's that's an empirical question, then that's something about which perhaps we should worry but it doesn't appear to be all that chaotic. At least in the medium term I remember everyone alive saying invading Iraq is a really stupid thing to do. And there are a few people with vessels who said oh no it won't be it will be awesome and of course what happens Middle East is on fire so I think

Unknown Speaker  1:15:25  
you're definitely not You're not going to be there are some different considerations will impose different temporal constraints on our attention so climate change is a really long term one you know that strain of thought has not had the greatest rate of success in the history of our species I charted Give me one second I'm here Sure yeah okay. Evening okay Okay we're back Alright

Unknown Speaker  1:17:34  
Okay, here we go. What has any of this got to do is computer science ever everything he is just one example some of you may have seen it before but we're about to take it to Aris slightly surreal hey you can see where this is going.

Unknown Speaker  1:18:40  
Okay,

Unknown Speaker  1:18:43  
here we have the infamous trolley problem originally to Philip a foot one of the greatest moral philosophers who've ever lived. I mean have heard thank or blame, thought experiment. Now in America, great to have my family is American I can say what I want about a man for reasons that escaped me. They will train cars trolleys, I don't know why. Maybe because that's what they were called in mines or something. So it's known as the runaway trolley problem, just the trolley problem. It's a train carts a big train is heavy.

Unknown Speaker  1:19:15  
And what it's going to do is to travel down the dotted line

Unknown Speaker  1:19:27  
on this dotted line, on this train track on this delta on this train track, we have

Unknown Speaker  1:19:35  
unlucky person, right.

Unknown Speaker  1:19:39  
And they're going to be killed by the tree. And just to to head off the smart aleck question they absolutely do not want to die. And they're suffering greatly at the thought of die and they will be extraordinarily happy. If you were to pull this lever, this is YOU OKAY. To divert the runaway train car, vaguely statistically looking forward to killing officers out there, so that it instead runs across the top track. Now, who thinks that the right thing to do in this situation, all things being equal is to pull this lever so as to divert the runaway train car and to save the life of the person who would die? Otherwise? It's not a trick question. Because I would have put your hands on all of you. It's supposed to be objects, right? That's why they're all these horrible examples in ethics because they're supposed to be for the most part uncontroversial. Yes. This person gains living, say. So the question is, what if this person is a psychopath? And they gain they they gain pleasure, but it's not it's you? Right? Right. That's it. This is my thought experiment. I get to make up how it works, right? I get to make up how it works. Now.

Unknown Speaker  1:21:16  
What is

Unknown Speaker  1:21:25  
there's a happy person on the top track you you don't know either than they're just two people who you happen to know for whatever reason are equally upset at the idea of dying and equally happy at the idea of living should you pull the lever

Unknown Speaker  1:21:48  
should you not?

Unknown Speaker  1:21:51  
No, no, because there's no way out of this. No, you can't break a lever you can't derail the train. There's none. There's no you know, there's no Superman in the train. You can't do any of this stuff. Right? Yes. Now that the same age Yeah, there for all intents and purposes qualitatively the same, yes. Can you flip a coin? Maybe? Maybe that is actually the right thing to do. I mean, you're doing nothing somebody will die you act that somebody will die. There is an argument against intervening against imposing diluting the situation with your agency if all things are equal, and so on, but flipping a coin might be one way to do it. But then the concern is you you could have just stood there and done nothing Why did you interfere? You kill my child because you flip the coin that What did you do? But then somebody make you watch No Country for Old Men at gunpoint in an impressionable age? Why are you flipping a coin and decided they're also to respond? This is supposed to be genuinely horrible, and we're sort of supposed to go Oh, God, I don't know. Like this is bad. Maybe I would flip a coin maybe I wouldn't maybe I would just be paralyzed in moral terror right off the back

Unknown Speaker  1:23:18  
say again

Unknown Speaker  1:23:26  
don't pay severance pay as well right. Did the suggestion was to tell them tell them to play scissors paper that that could be funny not obviously morally sanctified, however, yes. Yeah, absolutely. I think if you're utilitarian you just be paralyzed here. Right ride that's that's the point. And I can see where these if you've looked at the trolley problem before the situation it gets more and more horrible. Like, what if you know, there are two people here should you pull the lever now and you'll save two people, but you'll kill one other person who thinks you should pull the lever

Unknown Speaker  1:24:26  
who thinks you shouldn't

Unknown Speaker  1:24:29  
ever see into other students because you're interfering now and it's not up to you to kill this person? Like what do you know? These two are terribly unlucky, but who am I? Yeah, it's like you're playing God. That is the concern right? Now watching

Unknown Speaker  1:24:53  
Okay, there are five. So with no faces, there are five people here. Do you think you should pull the lever now? Ah, who thinks you still should not? Steal? Should not? Okay? What if there are 1000 people here? And they're all convicted racists and murderers? Right. And this is an award winning immunologist on the cusp of developing a cure to a whole bunch of diseases that are terrible, and we don't like them whatever they are, should you pull the lever, then

Unknown Speaker  1:25:32  
what if

Unknown Speaker  1:25:34  
the situation is exactly the same, except now this person is the awesome immunologist. And this person is a serial killer. If you do nothing, right, the immunologists will die, but if you pull the lever, the serial killer will die. And even all of us will live. Who would pull the lever then? We would not. I got to see some nods holding up, don't, don't let them get it down. The truth is not a popularity contest, okay? This is suppose suppose this person's going to die? If you do nothing, right, but they happen to be a really, really good friend of yours. And you don't even know this person. So you can save your friend's life at the cost of killing a stranger, who would pull the lever then and thinks it's the right thing to do?

Unknown Speaker  1:26:54  
Let's be friends that you who would not pull the lever and say that that is the wrong thing to do?

Unknown Speaker  1:27:04  
keeping my eye on you three that I just made? Yeah. Yes, yeah, in a brief attempt that levity that usually fails. Yeah. The to be serious for a moment. The criticism for of utilitarianism is that it leaves no room for friendship, and loyalty, and honor and love, and instead reduces interpersonal relationships and moral obligations to mere calculations of utility. But if that is how we've always treated each other, will we ever have had strong communities, or strong cultures or strong civilizations? bias towards an in group has been the cause of an awful lot of terrible things throughout the history of our species. But arguably, and not not obviously incorrectly, it's also been the very thing that has allowed our species to flourish in ways that it would not have if we had not shown preferential bias to to those within our circle as opposed to those without and the thought those on the outside of us of those without you know what I mean, terrible times. I will do so the worry here is that what utilitarianism would say is that suppose there are two people here that I don't know. Right? And suppose that the train is going to kill them. I like a pull the lever to save their lives at the cost of one. Maybe the three will make it with three strangers right? Up. Here's one of my dearest friends. utilitarians wasn't joking when they said we shouldn't be biased. The same the right thing to do morally speaking is to pull the lever hit. Somebody people are okay with that. And other people are not. It does lead to some consequences that in some situations are extraordinarily difficult. What does any of this got to do?

Unknown Speaker  1:29:48  
With computer science?

Unknown Speaker  1:29:53  
The Runaway trolley is a train the runaway rail car is a train that is moving autonomously, it is hurtling forward under its own momentum on a track, you can't get off the track, but you can move in all autonomous vehicles, that is self driving cars are for all intents and purposes, runaway rail cars, whose navigation protocols that have been programmed into them by you, ah, the tracks. And every switch pulling decision, every time there is a decision to make. So, the collision avoidance protocols that are programmed into an autonomous vehicle, are the very protocols that the decide that do decide whether or not the switch should be pulled at every instance. But here's the catch. You are always you are one of these people on the track, every time you get in an autonomous car, because your life is on the line, you are in the vehicle. So now think about how confident you would be getting into an autonomous vehicle. If it had been programmed by a ruthlessly unbiased, utility utilitarian. This is a car that is going to kill you, every single time. Avoiding doing so would result in the deaths of more than one other person. Would you still put yourself in that car?

Unknown Speaker  1:31:54  
Who wouldn't?

Unknown Speaker  1:31:58  
There's no way I know the things is going to kill me every time and thinks that there are three kids on the street or something. And there's got a choice between a head on collision with a truck and some kids on the side of the road. And that sounds really good abstractly, until you're the person in the car, then all of a sudden, if our own lives are at stake, if we're one of the people on the track, we might have much stronger opinions about what it is that this person over here should do. I mean, frankly, even if if I was having a bad day, and I wasn't feeling particularly sacrificial, and the train was heading towards me, and I knew there was another person up here who was a stranger, and this person could hear me, I would be probably trying pretty hard to talk them into pulling the lever. I like to think that I just take one for the team. But I think as I was facing imminent death, I'm like, I'll buy you a beer afterwards. It'll be fine. Just pull the lever, you know. But now suppose that, you know, there are three people here. Three strangers, I don't know. But I'm on this track. With my young child. Right now, I'd be trying really hard to get this person to pull the lever. Right, but the utility utilitarian level would. But what if I'm in the car with my children? How are we going to decide what protocols should be in place to avoid collisions in an autonomous car? Are you saying the immediate traction that the trolley problem and the utilitarian arguments that motivate discussions around it, as with the very sorts of decisions that are and arguments that programmers are having right now, and people responsible in companies, tech companies all over the world are having right now when it comes to collision avoidance procedures for autonomous vehicles. That's just one particular one intersection of everything we've talked about right now, which is all fun and games and philosophy, and the sorts of decisions that you might actually have to make in your careers when you leave if this is the area in which you're getting involved in. And every week, there'll be every week and every lecture will be a whole bunch of points of contact of professional business life and actual computer scientists live out there and industry. This is our first one. Is everyone seeing it? There's a connection here pretty clear. Are there any questions about this one? And also my only goal here is to have all of you worry now that all of a sudden this is really serious. No. Right? And if we get this wrong, my autonomous vehicles gonna kill me. The problem is what if all the time As vehicles are performing under the same protocols, you think I know we'll have them, we'll program them all to avoid the death of the occupants at all costs. But the the great thing about humans is that we're unpredictable. The great thing about computers is that they're predictable. So it's really hard to predict what would happen if a huge number of cars at an intersection if something goes wrong with the lights are all trying to avoid a collision at once. And they're all programmed to avoid harming the occupants at all costs. And there are like 50 vehicles, who knows what would happen then, is very hard to let actually physically in the world. Because now you're talking about not just one runaway trolley, but 10s of 1000s, or even hundreds of 1000s across the city. But one at any time. This gets this gets tricky. Now, it's tricky to predict mathematically. Yes, I had a question, say, deal with that one personal track? And, yeah, what happened to my child, let's say you have a child, and the trolley is going on to one thing. And you can switch it to the top at any time, how many people would need to be at the top, we need to sacrifice itself? So the question is for those remotely, how many people would need to be at the top? For me to sacrifice myself? I think you mean me personally, actually, I think it really just depends on my mood on any given day. And how old I was, at what point in my life, it had been there when I was much younger, I probably wouldn't have cared as being hysterical. And now I like to think I might just go off, you know, I've had a pretty good life. You know, this other person, they've done nothing to me, I like to think that I would do that. I think, you know, if there were, if there were 10? I think I definitely do that. I don't know exactly where the line is. But the argument I had with friends, this is what nerds do in Oxford, right? The argument we had in us is the difference between science fiction and fantasy. What's the difference? So I'd love to stop to go down to the bookstore, they put them all in piles, we're going to make two pretty similar piles and a couple of unknowns in the middle. I'm sure you could find some borderline case there is that you hear this form of argument all the time along the lines of but where would you draw the line. And that's supposed to be this devastating response. But your response to that response, which is really tried should always be the same. It's like, firstly, I don't know exactly where the line is, right. But I know this other thing is definitely on the wrong side of it. And that's all that matters here. And I don't need to know where the line is to know where things lie on either side of it for the look for the vast majority of cases. So I think 10 or up is definitely the other side of the line. If I was feeling really like, look, I don't know, homesick, I love him a partner, maybe 10 Maybe it'd be their own lucky day, I come home and be like, I I can't get a bad food today. But but I'm still here. So we need to talk about this, it's gonna be impacted, I don't know. And then if I had children, I was responsible for people. Like, it's really tricky. People do make all the time. Remarkably courageous decisions in these sorts of situations, people are incredibly resilient, I think morally, and, and, and emotionally, and can do all sorts of remarkably basic things, especially in terms of times of natural disaster, and war. And these sorts of things. Hopefully, the whole point of modern civilization is that the likelihood of us being in this sort of situation, when we have to make these sorts of decisions is sort of minimized, why we have standards, things are hopefully not falling down all the time. For the most part, they're not opening gun battles on the streets and these sorts of things in Australia, and we've got a long way to go. And also parts of the world, we've got a long way to go. But we're generally we are getting better at being humans as we go along. I think the mark of that is just the one thing for which I think we really do have the thank consequentialist ethics of just how it is that suffering has been minimizing suffering is now taken. Really, really, really seriously. You know, when I was a kid, what people even in Australia were just allowed to do to each other and it was okay. If it was an inside a man's house. It was his business and no one else could they could do anything was terrible. That doesn't cut it anymore. You notice like no, you can't carry on like this. You're being a terrible person. You're creating suffering, no matter what antecedent beliefs you if you just can't do it, then we're becoming slowly and hopefully increasingly kinder and more gentle as a species is incredibly tenuous societies can it all All sorts of wonderful, wonderful things then before you know it, that's all going to hell in a handbasket. But we're not doing too bad. Any other questions? Yes. I think we're, I think we are becoming that works. But more complicated

Unknown Speaker  1:40:35  
than that. So then the question is, it looks like utilitarianism has something positive to be said for it. However, it is a little bit complicated. So might not it'd be better if if I heard you, right? If we had a system of rules that everybody had to follow, and then there'd be less time wasted calculating utility? Yeah, absolutely. The danger, the The one concern there is that few things have been more dangerous in history than people who are convinced of their own moral superiority. The idea that I know what's good for you hasn't always worked out terribly well in the past, and as usually has very often resulted in a huge amount of horrific suffering. Although, you know, I do know utilitarians professional utilitarians they get paid to utilitarian at people. It's what they do. They write books about it, and they hang out with Peter Singer, I'm serious. And they say without a without batting an eye, they're totally serious. The right thing to do is an empirical matter. And for all empirical matters, there are people who are experts. And we defer to those experts. When I say I am an expert, when it comes to calculating utility and predicting it. I'm an expert to knowing what the good when it comes to knowing what the good is, therefore, people just did what I said the world would be a better place morally, and people would be much happier. And what's more, if I had the opportunity to force people to do what I said, I would take that opportunity and what's more, by my own moral lights, I will be morally obligated to do so. So utility, utilitarians can be tempted to, to towards the very same idea. Namely, I'm going to calculate utility outcomes for all of these actions. And then I'm going to make all the prescriptions and I know way more about this than you do. What Are any of you to doing think any of you like doing thinking you know how to work out what the right thing to do is a great utilitarian person, I'll tell you what to do. Because I've spent 30 years thinking about it. You defer to your dentist you should defer to professional ethicists to? And then I'll check it every time you serious? That's a really good question. Yes.

Unknown Speaker  1:43:10  
What is the trolley?

Unknown Speaker  1:43:16  
The trolley problem matters because it brings into stark relief, the very considerations that we need to bring to bear to try and work out what the right solution is. The trolley problem matters because we're actually making these decisions. Now, especially with regards to autonomous vehicles. The trolley problem matters because it's a it's another way of talking about triage. This is a discussion that came up during the pandemic, where we had like doctor after doctor all over the world and hospitals ever saying, look, there are so many sick people coming in now, I can't save them all. I have to make all these decisions about who will treat and who we will not treat is basically from a medical perspective, like a warzone. But I'm not an Army doctor. I've never thought about triage at this scale. Before I'm going home every day, with all of these deaths on my conscience. This is not the sort of thing that we train for. When we come to doctors and hospitals. It's like a like an earthquake has been going on for years now. Well, somebody who's not me, please make a decision about what we're supposed to do here is supposed to just bring bring to light, forced moral decisions that will have consequences. And the trolley problem itself is fantastic. It's also supposed to bring to light some of the ugliest sides of utilitarianism, like sacrifice your friends, for the many. And that's something which some people think is great, but other people are really concerned about this all this is still an open question. I wish I could just say oh, by the way, they'll go and do this and you'll be good people. That's not how that's not how morality works.

Unknown Speaker  1:44:50  
Yes, in the case of autonomous driving, do you know what real world industry actually do? Do they protect the passenger or do they protect people outside? Oh,

Unknown Speaker  1:44:59  
that's perfect. i Terry at the moment. So I don't know Yes,

Unknown Speaker  1:45:03  
isn't quite on my mind that at the end, it's kind of hard to tell where to put it. But we're talking about like professional ethicist and setting right. So is the point of this course to learn what this course is on ethics is for to be able to evaluate ethical questions and sort of come up with your own

Unknown Speaker  1:45:21  
your own perspectives, the latter. So the point of this course is to not the whole course, is to enable all of you in virtue of your increase conceptual literacy, with all the attendant material, when it comes to ethical reasoning, to engage with all sorts of ethical claims on their own terms, as an informed participant, in the same way that you can't really engage in scientific arguments, if you don't understand the math, because you don't understand what people are really saying. Like non sciency people, when they go to a TED talk, and they have a speech, or like, I learned all about quantum physics last night, like no you didn't, you went to a thing for an hour, you don't know anything about tensor operations to stop it. To engage properly in ethical discussions, you need to know the entire verbiage of ethical theory, so that you can give reasons, principle reasons for why it is that you are making the ethical statements that you are, and why it is that you are evaluating the ethical judgments in the way that you are. And when you ask a lot of people in companies that I've seen this, how did you come up with your How did you design your code of ethics for your comments, like, oh, we just made up? You know, that was the one thing coming from anything principal at all. And that's absolutely not. And we're going to look at that when we get to the lecture in ethics washing, which is just where you sort of pretend to be really moral, because you use the word ethics, lots, okay. Now before, that's the end of the ethics stuff, before you go, I'm happy, I've got a whole hour to kill. I don't know if there's another lecture in here. But if there is, we'll go outside. I'm happy to stick around for a whole hour, then we've got the rep meetings in an hour with the student reps. All of us pull him ah, by in a moment, just one sec. No, actually, no, actually, Paula, why don't you come up now, I think we must pull up one of your awesome student reps is going to tell you about student rep things. And then I'm going to tell you all sorts of fascinating bits and pieces about how this the course will be organized, and assessments and so on. But more importantly, before then, all the things you take it away. Hi,

Unknown Speaker  1:47:42  
I'm Paul, I'm one of the st reps we also have. Ada is another one of the three reps doing this course. So if you have any complaints about the course, you can definitely contact us. We'll put the email and also the website. We can like go and write an anonymous form. Probably we'll put it in the forum, and maybe get up pins as an announcement. Right? Um, yeah. So if you have any, like issues with the course try and just probably try and figure it out with Cephus. I'm sure it will be very reasonable. I think so it kind of your job to be reasonable. So, but if you want to be anonymous, then definitely contact us. And yeah, or if you just think it's easier to just go through us. We we have asked to record meetings with said, but we also have them with other. I think Jess also comes to them. So we'll just bring it up with the head of school and try and resolve anything. Cool. Yeah. Make sure to contact us and don't just complain in the messenger group chat. You can. You can also just like ping us on social media, you know, if you never really asked,

Unknown Speaker  1:48:39  
but thank you so much. Awesome. Okay, um, so, inevitably, someone is about to ask me, Oh, will you post your lecture notes?

Unknown Speaker  1:49:05  
In case it hasn't been perfectly obvious, I have been winging it so far. There has not been winging it is because I'm at least passingly familiar with the material haven't been worrying about writing about I'm teaching it for 19 years. Now, here's the thing. There are no separate tasks in the tutorials. It's not like there's a whole second point is there's all the secondary stuff you have to do in the tooth. The point of the tutorials is to reinforce this material and to revise it and to take it further the point of the lectures is to just equip you sufficiently Okay, so that when you get to the choose, you can go nuts. Now taking notes. Notes is a skill. It is a difficult skill. It is is one that we learn only by practice. When we begin, usually we try and write down everything that someone is saying. And then you realize that you have listened to and understood absolutely none of it. Right, then when you realize that you don't take any notes for a couple of years, and that's also terrible. And the right thing to do is to take just enough notes that you can remind yourself of everything that happened, whilst at the same time not distracting yourself from what's going on in the room. So here's a hint. If your notes are taking up longer than more than a single a4 page, you took too many. What your tutors are going to do with you, in the tutorials is help you reconstruct the content of the lecture on a single page in notes. And one of the the tutorial exercises that you're going to be doing, you're not going to be getting marked up. And the whole point is that I want you to learn the skill needed to help each other to do it is to learn how to take notes properly. Note taking proper Note taking is a life long skill. If you get to the workplace and some board meeting somewhere, and at the end of it, you say Can somebody click the deck, can you give me your notes, you'll probably get sacked a lot that no one's making notes for you out there on the wall. Instead, one of the things we're going to learn to do here together is to take the notes, but I'm going to do also. But after the fact is I will send around my own reconstruction of the lectures against which you can match your own. And those of you all done together in the tutorials. The way I make notes when I'm doing it is I don't write anything down in sentences. At least a whole sentence at all.

Unknown Speaker  1:52:01  
I still use

Unknown Speaker  1:52:07  
my last, I would do something like okay, ethical arguments. Right, that was the first thing. And then and then you take off. And then my whole point. And then often I'm thinking I might need to make a few little notes around the edge. The goal is I want to be able to just look at the nodes in my mind map that will fit on an a4 page in such a way that there's just one page of nodes and then with arrows between them. And that's sufficient for me to reconstruct the entire narrative in my head, as I read through the notes, and then I know I've understood it. The point of notes is to help us understand something, it's not to create a sort of external device that protects us from understanding it. Yes. What if taking a lot of notes. Like we can spend time with friends and it makes our group of friends. Oh, we get a bad grade? Ah, so the question is, what if the the taking of the notes causes a huge amount of unhappiness? Right. Well, that's fine. Because next week, we're going to be content ethicist. And we'll see how consequences don't matter. All that ethical reasoning. Right. Okay. Yes. No, there are no tutorials in first week. No dolls in first week? No, no, no, no, no. All right. Your first essay will be doing fifth week, I'll put the questions up, either by the end of this week, or by Monday at the latest. That first essay it will be they have a whole bunch of different questions you can choose, it will be 2000 words long, it will be worth 50% of your final mark. But no one is going to fail unless you're nuts, because I'm going to tell you what the answers are to the questions. But that's not going to make it quite as easy. As you see. One of the things I want us all to learn together is how to write a great essay. And then what I'm going to do is go through the questions because I think a great answer to this question would cover these sorts of things, then how you do it is is up to you. You know, none of these assessments are here to trip you over. No one unless you fell the hands something No, you just lose your minds is going to fail. Right? What I want you to do is come on having learned a bunch of stuff. The assessments are there for you to demonstrate what you have learned. Yes. Oh, yeah. So the question is, what are we doing for the tutorials that are on next Thursday? Because it's a public holiday? The answer that question is I have no idea. But your tutors and I are all meeting tomorrow. We're going to work it out together. And then I'm going to let you know why couldn't have been on a Friday. What was Thursday? It's it's a weird, everyone's gonna take Friday off anyway, right? It's Australia. Of course they are. That's what we do. It was so weird that I got sick on the Friday, I had four days off. Okay. Okay, so time is pretty much up, there might be another lecture. Nice. I'll stop hacking up. But I'm happy to stick around for as long as we need to for other questions, and I'll keep my eyes on zoom as well. Thanks, everyone. Ah, there's more chocolate if people would still like chocolate. Next week, there'll be chocolate. If anyone would like to take chocolate. Otherwise, bring it up, and they may have chocolate Aha, so come over here to the line. The question was, what's an empirical question? An empirical question is a question about the way that things are in the world in the empirical, external world. So an empirical question is about the external world, as opposed to something like, what is one plus one? That is not an empirical question, because I'm going to work out the answer to that, because by thinking about it really carefully, and empirical question is one, the answer to which we can never work out no matter how smart we are, because we need evidence from the world. Standard lyrical evidence? That's an empirical question. Good. Thank you. All right.

Unknown Speaker  1:56:31  
Thanks, I terms of the trolley problem, it could be in the example of who's responsible to whatever decision you

Unknown Speaker  1:56:42  
if you're the one making the decision, you're responsible. Oh, you mean? Oh, you made the autonomous car? Oh, that's a whole other kind of one. Yeah, exactly. So there will be like, a lot of different body building extra engineered. Yeah. There'll be the design and building. It will be the accuracy of the computer. But at the end of the day, look at the broad picture, because actually, how that's such a difficult question to answer, and I've done a lot the answer is it's such a great one, like how does responsibility distribute over a whole the whole collectives Luciano Floridi. has a great paper on this as about moral responsibility. Will you remember that email me about this and put it on the forum? And I'll finally China's paper and I'll post it. Yeah, so I'm not sure if we'll be getting to it in the lectures, specially. Interviews. versation because I got it right. Yes. All right.

Transcribed by https://otter.ai
